# ğŸ§  Reinforcement Learning Algorithms

> Analysis of RL algorithms for Terra Scout.

---

## ğŸ“‹ Overview

This document analyzes reinforcement learning algorithms suitable for the Terra Scout diamond-finding task.

### Task Characteristics

| Characteristic    | Value                          | Implication                     |
| ----------------- | ------------------------------ | ------------------------------- |
| Observation Space | High-dimensional (images)      | Need function approximation     |
| Action Space      | Discrete + Continuous (camera) | Hybrid action handling          |
| Reward Density    | Sparse (diamond rare)          | Need exploration strategies     |
| Episode Length    | Long (thousands of steps)      | Need temporal credit assignment |
| Environment       | Stochastic, complex            | Need robust learning            |

---

## ğŸ† Algorithm Comparison

### Quick Comparison

| Algorithm  | Type            | Sample Efficiency | Stability | Complexity | Recommendation |
| ---------- | --------------- | ----------------- | --------- | ---------- | -------------- |
| **PPO**    | Policy Gradient | Medium            | High      | Low        | â­ Primary     |
| **DQN**    | Value-Based     | Medium            | Medium    | Medium     | â­ Alternative |
| **A2C**    | Actor-Critic    | Low               | Medium    | Low        | Baseline       |
| **SAC**    | Actor-Critic    | High              | High      | High       | Future         |
| **IMPALA** | Distributed     | Very High         | High      | Very High  | Not for MVP    |

---

## 1ï¸âƒ£ PPO (Proximal Policy Optimization)

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PPO                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Actor     â”‚    â”‚   Critic    â”‚    â”‚   Clip      â”‚     â”‚
â”‚  â”‚  (Policy)   â”‚    â”‚  (Value)    â”‚    â”‚  Objective  â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                  â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚                   Stable Policy Updates                     â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. Collect trajectories using current policy
2. Compute advantages (how much better/worse than expected)
3. Update policy with clipped objective (prevents large updates)
4. Update value function
5. Repeat

### Key Equations

```
Policy Objective:
L^CLIP(Î¸) = E[min(r_t(Î¸) * A_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ) * A_t)]

Where:
- r_t(Î¸) = Ï€_Î¸(a|s) / Ï€_Î¸_old(a|s)  (probability ratio)
- A_t = advantage estimate
- Îµ = clip parameter (typically 0.2)
```

### Hyperparameters

| Parameter       | Typical Value | Description                |
| --------------- | ------------- | -------------------------- |
| `learning_rate` | 3e-4          | Step size for updates      |
| `n_steps`       | 2048          | Steps before update        |
| `batch_size`    | 64            | Minibatch size             |
| `n_epochs`      | 10            | Epochs per update          |
| `gamma`         | 0.99          | Discount factor            |
| `gae_lambda`    | 0.95          | GAE parameter              |
| `clip_range`    | 0.2           | Clipping parameter         |
| `ent_coef`      | 0.01          | Entropy bonus              |
| `vf_coef`       | 0.5           | Value function coefficient |

### Pros & Cons

| Pros                            | Cons                                 |
| ------------------------------- | ------------------------------------ |
| âœ… Stable training              | âš ï¸ On-policy (less sample efficient) |
| âœ… Simple to implement          | âš ï¸ Sensitive to hyperparameters      |
| âœ… Works well with images       | âš ï¸ Can plateau early                 |
| âœ… Good default choice          |                                      |
| âœ… SB3 implementation available |                                      |

### Suitability for Terra Scout

```
Overall Suitability: â­â­â­â­â­ (5/5)

âœ… Handles image observations well
âœ… Stable with sparse rewards
âœ… Works with hybrid action spaces
âœ… Well-tested in MineRL competitions
âœ… Good SB3 implementation
```

---

## 2ï¸âƒ£ DQN (Deep Q-Network)

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DQN                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚   Q-Net     â”‚    â”‚   Target    â”‚    â”‚   Replay    â”‚     â”‚
â”‚  â”‚  (Online)   â”‚    â”‚   Q-Net     â”‚    â”‚   Buffer    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚         â”‚                  â”‚                  â”‚             â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚                    Q-Value Learning                         â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. Store experiences in replay buffer
2. Sample random batch from buffer
3. Compute target Q-values using target network
4. Update online network to minimize TD error
5. Periodically update target network
6. Select actions using Îµ-greedy

### Key Equations

```
Q-Learning Update:
Q(s, a) â† Q(s, a) + Î± * [r + Î³ * max_a' Q(s', a') - Q(s, a)]

Loss Function:
L(Î¸) = E[(r + Î³ * max_a' Q_target(s', a') - Q(s, a; Î¸))^2]
```

### Hyperparameters

| Parameter       | Typical Value | Description                  |
| --------------- | ------------- | ---------------------------- |
| `learning_rate` | 1e-4          | Step size for updates        |
| `buffer_size`   | 1000000       | Replay buffer size           |
| `batch_size`    | 32            | Minibatch size               |
| `gamma`         | 0.99          | Discount factor              |
| `epsilon_start` | 1.0           | Initial exploration          |
| `epsilon_end`   | 0.05          | Final exploration            |
| `epsilon_decay` | 0.995         | Decay rate                   |
| `target_update` | 10000         | Steps between target updates |

### Pros & Cons

| Pros                                | Cons                               |
| ----------------------------------- | ---------------------------------- |
| âœ… Sample efficient (replay buffer) | âŒ Discrete actions only           |
| âœ… Off-policy learning              | âš ï¸ Overestimation bias             |
| âœ… Stable with target network       | âš ï¸ Struggles with high-dim actions |
| âœ… Well understood                  | âš ï¸ Needs action discretization     |

### Suitability for Terra Scout

```
Overall Suitability: â­â­â­ (3/5)

âœ… Good sample efficiency
âœ… Well-understood algorithm
âš ï¸ Requires discretizing camera actions
âš ï¸ May struggle with complex action space
âŒ Not ideal for continuous actions
```

---

## 3ï¸âƒ£ A2C (Advantage Actor-Critic)

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         A2C                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   Actor     â”‚              â”‚   Critic    â”‚              â”‚
â”‚  â”‚  (Policy)   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  (Value)    â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   Advantage  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                            â”‚                      â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚                       â”‚                                     â”‚
â”‚                       â–¼                                     â”‚
â”‚              Synchronized Updates                           â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. Run policy for n steps
2. Compute returns and advantages
3. Update actor using policy gradient with advantage
4. Update critic using value loss
5. Repeat synchronously

### Key Equations

```
Policy Gradient:
âˆ‡_Î¸ J(Î¸) = E[âˆ‡_Î¸ log Ï€_Î¸(a|s) * A(s, a)]

Advantage:
A(s, a) = Q(s, a) - V(s) â‰ˆ r + Î³V(s') - V(s)
```

### Suitability for Terra Scout

```
Overall Suitability: â­â­â­ (3/5)

âœ… Simpler than PPO
âœ… Lower variance than REINFORCE
âš ï¸ Less stable than PPO
âš ï¸ Lower sample efficiency
```

---

## 4ï¸âƒ£ SAC (Soft Actor-Critic)

### Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         SAC                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Actor     â”‚   â”‚  Critic 1   â”‚   â”‚  Critic 2   â”‚       â”‚
â”‚  â”‚ (Stochastic)â”‚   â”‚  (Q-value)  â”‚   â”‚  (Q-value)  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                  â”‚                 â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                            â”‚                                â”‚
â”‚                            â–¼                                â”‚
â”‚              Entropy-Regularized Learning                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Suitability for Terra Scout

```
Overall Suitability: â­â­â­â­ (4/5) - Future consideration

âœ… Very sample efficient
âœ… Automatic entropy tuning
âœ… Great for continuous actions
âš ï¸ More complex implementation
âš ï¸ May be overkill for MVP
```

---

## ğŸ¯ Recommendation for Terra Scout

### Primary Choice: PPO

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RECOMMENDED: PPO                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Reasons:                                                   â”‚
â”‚  â”œâ”€â”€ Proven in MineRL competitions                         â”‚
â”‚  â”œâ”€â”€ Stable training with sparse rewards                   â”‚
â”‚  â”œâ”€â”€ Handles image + discrete/continuous actions           â”‚
â”‚  â”œâ”€â”€ Excellent SB3 implementation                          â”‚
â”‚  â”œâ”€â”€ Good documentation and community support              â”‚
â”‚  â””â”€â”€ Reasonable sample efficiency                          â”‚
â”‚                                                             â”‚
â”‚  Configuration:                                             â”‚
â”‚  â”œâ”€â”€ Policy: CnnPolicy (for image observations)            â”‚
â”‚  â”œâ”€â”€ n_steps: 2048                                         â”‚
â”‚  â”œâ”€â”€ batch_size: 64                                        â”‚
â”‚  â”œâ”€â”€ learning_rate: 3e-4                                   â”‚
â”‚  â””â”€â”€ clip_range: 0.2                                       â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fallback: DQN with Discretized Actions

If PPO struggles, consider DQN with:

- Discretized camera movements
- Simplified action space
- Larger replay buffer

### Future Exploration: SAC

After MVP, consider SAC for:

- Better sample efficiency
- More exploration via entropy
- Continuous camera control

---

## ğŸ“Š Training Strategy

### Curriculum Learning

```
Stage 1: Simple Navigation
â”œâ”€â”€ Flat terrain
â”œâ”€â”€ Visible target
â””â”€â”€ Dense rewards

Stage 2: Underground Navigation
â”œâ”€â”€ Cave systems
â”œâ”€â”€ Hidden targets
â””â”€â”€ Shaped rewards

Stage 3: Diamond Finding
â”œâ”€â”€ Full environment
â”œâ”€â”€ Sparse rewards
â””â”€â”€ Full complexity
```

### Reward Shaping Schedule

```
Early Training:
â”œâ”€â”€ Heavy shaping (guide exploration)
â””â”€â”€ Frequent rewards

Mid Training:
â”œâ”€â”€ Reduce shaping
â””â”€â”€ Transition to sparse

Late Training:
â”œâ”€â”€ Minimal shaping
â””â”€â”€ Pure task reward
```

---

## ğŸ“ Related Documents

- [REWARD_DESIGN.md](REWARD_DESIGN.md)
- [REFERENCES.md](REFERENCES.md)
- [../architecture/SYSTEM_OVERVIEW.md](../architecture/SYSTEM_OVERVIEW.md)
