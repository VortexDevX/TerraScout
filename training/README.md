# ğŸ‹ï¸ Terra Scout Training

> Training scripts, configurations, and experiment management.

---

## ğŸ“ Structure

```
training/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py           # Main training script
â”‚   â”œâ”€â”€ evaluate.py        # Evaluation script
â”‚   â””â”€â”€ export_model.py    # Model export utilities
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ training_config.yaml    # Training hyperparameters
â”‚   â””â”€â”€ hyperparameters.yaml    # Model hyperparameters
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ train_kaggle.ipynb      # Kaggle training notebook
â”‚   â””â”€â”€ analysis.ipynb          # Results analysis
â”œâ”€â”€ checkpoints/           # Saved model weights
â”œâ”€â”€ logs/                  # Training logs (TensorBoard)
â”œâ”€â”€ experiments/           # Experiment tracking
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸš€ Quick Start

### Local Training

```bash
# Activate environment
cd TerraScout
.\venv\Scripts\Activate.ps1

# Start training
python training/scripts/train.py

# With custom config
python training/scripts/train.py --config training/configs/training_config.yaml

# With overrides
python training/scripts/train.py --total-timesteps 500000 --learning-rate 0.0001
```

### Kaggle Training

1. Upload `training/notebooks/train_kaggle.ipynb` to Kaggle
2. Enable GPU accelerator
3. Run all cells
4. Download checkpoints when complete

---

## âš™ï¸ Configuration

### Training Config

```yaml
# training/configs/training_config.yaml

environment:
  name: "MineRLObtainDiamond-v0"
  max_episode_steps: 18000

algorithm:
  name: "PPO"
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64

training:
  total_timesteps: 1000000
  eval_freq: 10000
  save_freq: 50000
```

### Key Parameters

| Parameter         | Description         | Recommended |
| ----------------- | ------------------- | ----------- |
| `total_timesteps` | Training duration   | 1M+         |
| `learning_rate`   | Update step size    | 3e-4        |
| `n_steps`         | Steps per update    | 2048        |
| `batch_size`      | Minibatch size      | 64          |
| `eval_freq`       | Evaluation interval | 10000       |

---

## ğŸ“Š Monitoring

### TensorBoard

```bash
# Start TensorBoard
tensorboard --logdir training/logs

# Open browser
# http://localhost:6006
```

### Key Metrics

| Metric           | Good Sign         | Bad Sign        |
| ---------------- | ----------------- | --------------- |
| `episode_reward` | Increasing        | Flat/Decreasing |
| `diamond_rate`   | Increasing        | Zero            |
| `policy_loss`    | Stable/Decreasing | Exploding       |
| `entropy`        | Gradual decrease  | Rapid collapse  |

---

## ğŸ’¾ Checkpoints

### Automatic Saves

```
checkpoints/
â”œâ”€â”€ model_50000_steps.zip
â”œâ”€â”€ model_100000_steps.zip
â”œâ”€â”€ model_best.zip          # Best eval reward
â””â”€â”€ model_last.zip          # Most recent
```

### Loading Checkpoints

```python
from stable_baselines3 import PPO

# Load model
model = PPO.load("training/checkpoints/model_best.zip")

# Continue training
model.learn(total_timesteps=500000)
```

---

## ğŸ“ˆ Evaluation

```bash
# Evaluate trained model
python training/scripts/evaluate.py \
    --model training/checkpoints/model_best.zip \
    --episodes 100

# With video recording
python training/scripts/evaluate.py \
    --model training/checkpoints/model_best.zip \
    --episodes 10 \
    --record
```

---

## ğŸ“ Related Documentation

- [Training Guide](../docs/guides/TRAINING_GUIDE.md)
- [RL Algorithms](../docs/research/RL_ALGORITHMS.md)
- [Reward Design](../docs/research/REWARD_DESIGN.md)
