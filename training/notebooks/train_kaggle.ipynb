{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504a3d12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Terra Scout - Kaggle GPU Training\\n\",\n",
    "    \"\\n\",\n",
    "    \"Train the diamond-finding agent using Kaggle's free GPU.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Setup:**\\n\",\n",
    "    \"1. Enable GPU: Settings â†’ Accelerator â†’ GPU T4 x2\\n\",\n",
    "    \"2. Run all cells\\n\",\n",
    "    \"3. Download trained model when complete\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 1: Install dependencies\\n\",\n",
    "    \"!pip install stable-baselines3[extra] gymnasium torch numpy tensorboard\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 2: Check GPU\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"print(f\\\"PyTorch: {torch.__version__}\\\")\\n\",\n",
    "    \"print(f\\\"CUDA available: {torch.cuda.is_available()}\\\")\\n\",\n",
    "    \"if torch.cuda.is_available():\\n\",\n",
    "    \"    print(f\\\"GPU: {torch.cuda.get_device_name(0)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 3: Create simplified environment for Kaggle\\n\",\n",
    "    \"# (No Minecraft connection - simulated environment)\\n\",\n",
    "    \"\\n\",\n",
    "    \"import gymnasium as gym\\n\",\n",
    "    \"from gymnasium import spaces\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"\\n\",\n",
    "    \"class TerraScoutSimEnv(gym.Env):\\n\",\n",
    "    \"    \\\"\\\"\\\"Simulated Terra Scout for offline training.\\\"\\\"\\\"\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.action_space = spaces.Discrete(20)\\n\",\n",
    "    \"        self.observation_space = spaces.Box(-np.inf, np.inf, (35,), np.float32)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        self.max_steps = 2000\\n\",\n",
    "    \"        self.current_step = 0\\n\",\n",
    "    \"        self.y_level = 64\\n\",\n",
    "    \"        self.diamond_chance = 0.001  # Base chance\\n\",\n",
    "    \"        self.visited = set()\\n\",\n",
    "    \"        self.position = [0, 64, 0]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def reset(self, seed=None, options=None):\\n\",\n",
    "    \"        super().reset(seed=seed)\\n\",\n",
    "    \"        self.current_step = 0\\n\",\n",
    "    \"        self.y_level = 64\\n\",\n",
    "    \"        self.visited.clear()\\n\",\n",
    "    \"        self.position = [0, 64, 0]\\n\",\n",
    "    \"        return self._get_obs(), {}\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _get_obs(self):\\n\",\n",
    "    \"        obs = np.zeros(35, dtype=np.float32)\\n\",\n",
    "    \"        obs[0] = self.position[0] / 100\\n\",\n",
    "    \"        obs[1] = self.position[1] / 64\\n\",\n",
    "    \"        obs[2] = self.position[2] / 100\\n\",\n",
    "    \"        obs[3] = 1.0  # Health\\n\",\n",
    "    \"        obs[4] = 1.0  # Food\\n\",\n",
    "    \"        obs[5] = float(self.y_level <= -50)  # At diamond level\\n\",\n",
    "    \"        obs[6] = float(-59 <= self.y_level <= -54)  # Optimal Y\\n\",\n",
    "    \"        return obs\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def step(self, action):\\n\",\n",
    "    \"        self.current_step += 1\\n\",\n",
    "    \"        reward = -0.001\\n\",\n",
    "    \"        done = False\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Movement simulation\\n\",\n",
    "    \"        if action == 7:  # descend\\n\",\n",
    "    \"            self.y_level -= 1\\n\",\n",
    "    \"            self.position[1] = self.y_level\\n\",\n",
    "    \"            if self.y_level < 64:\\n\",\n",
    "    \"                reward += 0.01\\n\",\n",
    "    \"        elif action in [8, 9, 10]:  # mining actions\\n\",\n",
    "    \"            if self.y_level <= -50:\\n\",\n",
    "    \"                reward += 0.02\\n\",\n",
    "    \"                # Chance to find diamond at optimal Y\\n\",\n",
    "    \"                if -59 <= self.y_level <= -54:\\n\",\n",
    "    \"                    if np.random.random() < 0.005:  # 0.5% per mining action\\n\",\n",
    "    \"                        reward += 1000\\n\",\n",
    "    \"                        done = True\\n\",\n",
    "    \"                        print(\\\"ðŸ’Ž Diamond found!\\\")\\n\",\n",
    "    \"        elif action == 12:  # explore cave\\n\",\n",
    "    \"            if self.y_level <= 0:\\n\",\n",
    "    \"                reward += 0.01\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Y-level rewards\\n\",\n",
    "    \"        if self.y_level <= -50:\\n\",\n",
    "    \"            reward += 0.02\\n\",\n",
    "    \"        if -59 <= self.y_level <= -54:\\n\",\n",
    "    \"            reward += 0.05\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Exploration\\n\",\n",
    "    \"        pos_key = (int(self.position[0]), int(self.position[1]))\\n\",\n",
    "    \"        if pos_key not in self.visited:\\n\",\n",
    "    \"            reward += 0.01\\n\",\n",
    "    \"            self.visited.add(pos_key)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        truncated = self.current_step >= self.max_steps\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        return self._get_obs(), reward, done, truncated, {\\\"y_level\\\": self.y_level}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test environment\\n\",\n",
    "    \"env = TerraScoutSimEnv()\\n\",\n",
    "    \"obs, _ = env.reset()\\n\",\n",
    "    \"print(f\\\"Observation shape: {obs.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Action space: {env.action_space}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 4: Training with PPO\\n\",\n",
    "    \"from stable_baselines3 import PPO\\n\",\n",
    "    \"from stable_baselines3.common.callbacks import BaseCallback\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"class TrainingCallback(BaseCallback):\\n\",\n",
    "    \"    def __init__(self, verbose=0):\\n\",\n",
    "    \"        super().__init__(verbose)\\n\",\n",
    "    \"        self.episode_rewards = []\\n\",\n",
    "    \"        self.episode_count = 0\\n\",\n",
    "    \"        self.current_reward = 0\\n\",\n",
    "    \"        self.diamonds_found = 0\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def _on_step(self):\\n\",\n",
    "    \"        self.current_reward += self.locals.get('rewards', [0])[0]\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        if self.locals.get('dones', [False])[0]:\\n\",\n",
    "    \"            self.episode_count += 1\\n\",\n",
    "    \"            self.episode_rewards.append(self.current_reward)\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if self.current_reward > 500:\\n\",\n",
    "    \"                self.diamonds_found += 1\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            if self.episode_count % 100 == 0:\\n\",\n",
    "    \"                avg = np.mean(self.episode_rewards[-100:])\\n\",\n",
    "    \"                print(f\\\"Episode {self.episode_count}: avg_reward={avg:.2f}, diamonds={self.diamonds_found}\\\")\\n\",\n",
    "    \"            \\n\",\n",
    "    \"            self.current_reward = 0\\n\",\n",
    "    \"        return True\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create model\\n\",\n",
    "    \"env = TerraScoutSimEnv()\\n\",\n",
    "    \"model = PPO(\\n\",\n",
    "    \"    \\\"MlpPolicy\\\",\\n\",\n",
    "    \"    env,\\n\",\n",
    "    \"    learning_rate=3e-4,\\n\",\n",
    "    \"    n_steps=2048,\\n\",\n",
    "    \"    batch_size=64,\\n\",\n",
    "    \"    n_epochs=10,\\n\",\n",
    "    \"    gamma=0.99,\\n\",\n",
    "    \"    verbose=0,\\n\",\n",
    "    \"    device=\\\"cuda\\\" if torch.cuda.is_available() else \\\"cpu\\\",\\n\",\n",
    "    \"    tensorboard_log=\\\"./logs/\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training on: {model.device}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 5: Train!\\n\",\n",
    "    \"TOTAL_TIMESTEPS = 100_000  # Increase for better results\\n\",\n",
    "    \"\\n\",\n",
    "    \"callback = TrainingCallback()\\n\",\n",
    "    \"model.learn(\\n\",\n",
    "    \"    total_timesteps=TOTAL_TIMESTEPS,\\n\",\n",
    "    \"    callback=callback,\\n\",\n",
    "    \"    progress_bar=True\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nTraining complete!\\\")\\n\",\n",
    "    \"print(f\\\"Total episodes: {callback.episode_count}\\\")\\n\",\n",
    "    \"print(f\\\"Diamonds found: {callback.diamonds_found}\\\")\\n\",\n",
    "    \"print(f\\\"Average reward: {np.mean(callback.episode_rewards):.2f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 6: Save model\\n\",\n",
    "    \"os.makedirs(\\\"models\\\", exist_ok=True)\\n\",\n",
    "    \"model.save(\\\"models/terra_scout_kaggle\\\")\\n\",\n",
    "    \"print(\\\"Model saved to models/terra_scout_kaggle.zip\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 7: Evaluate\\n\",\n",
    "    \"env = TerraScoutSimEnv()\\n\",\n",
    "    \"model = PPO.load(\\\"models/terra_scout_kaggle\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"eval_episodes = 100\\n\",\n",
    "    \"rewards = []\\n\",\n",
    "    \"diamonds = 0\\n\",\n",
    "    \"\\n\",\n",
    "    \"for ep in range(eval_episodes):\\n\",\n",
    "    \"    obs, _ = env.reset()\\n\",\n",
    "    \"    total_reward = 0\\n\",\n",
    "    \"    done = False\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    while not done:\\n\",\n",
    "    \"        action, _ = model.predict(obs, deterministic=True)\\n\",\n",
    "    \"        obs, reward, terminated, truncated, info = env.step(action)\\n\",\n",
    "    \"        total_reward += reward\\n\",\n",
    "    \"        done = terminated or truncated\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    rewards.append(total_reward)\\n\",\n",
    "    \"    if total_reward > 500:\\n\",\n",
    "    \"        diamonds += 1\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Evaluation Results ({eval_episodes} episodes):\\\")\\n\",\n",
    "    \"print(f\\\"  Mean reward: {np.mean(rewards):.2f}\\\")\\n\",\n",
    "    \"print(f\\\"  Diamond rate: {diamonds}/{eval_episodes} ({100*diamonds/eval_episodes:.1f}%)\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Cell 8: Download model\\n\",\n",
    "    \"from IPython.display import FileLink\\n\",\n",
    "    \"FileLink('models/terra_scout_kaggle.zip')\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
